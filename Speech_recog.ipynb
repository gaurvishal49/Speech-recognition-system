{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inporting Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical# Hot encoding y\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import keras\n",
    "import h5py\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers import BatchNormalization\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdata = np.load('dev_data.npy',allow_pickle=True,encoding='bytes')\n",
    "Ydata = np.load('dev_labels.npy',allow_pickle=True,encoding='bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(arr,yarr, k):\n",
    "    k21 = 2*k+1\n",
    "    new_arr = np.zeros((arr.shape[0]-k21,k21,40))\n",
    "    for i in range(arr.shape[0]-k21):\n",
    "        new_arr[i,:,:] = arr[i:i+k21,:]\n",
    "        new_yarr = yarr[k:yarr.shape[0]-k-1,]\n",
    "    return new_arr, new_yarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata, ydata = align(Xdata, Ydata,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdata = xdata.reshape((xdata.shape[0], xdata.shape[1], xdata.shape[2],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "Ydata = to_categorical(lb.fit_transform(ydata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata, xtest, ydata, ytest = train_test_split(Xdata, Ydata, test_size=0.25, random_state=42)\n",
    "Xdata = []\n",
    "Ydata = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "with tpu_strategy.scope():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(13, 40,1)))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(138, activation='softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(xdata, ydata, epochs=30, batch_size=2048, validation_data=(xtest, ytest), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']# Set figure size.\n",
    "plt.figure(figsize=(12, 8))# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_accuracy, label='Training Accuracy', color='#185fad')\n",
    "plt.plot(val_accuracy, label='Validation Accuracy', color='orange')# Set title\n",
    "plt.title('Training and Validation Accuracy by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "plt.xticks(range(0,30,5), range(0,30,5))\n",
    "plt.legend(fontsize = 18);\n",
    "plt.savefig('accuracy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"final_modelV3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('final_modelV2.h5', compile = True)\n",
    "testx = np.load('test_data.npy',allow_pickle=True,encoding='bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligning the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_test(arr, k):\n",
    "    k21 = 2*k+1\n",
    "    new_arr = np.zeros((arr.shape[0],k21,40))\n",
    "    for i in range(k, arr.shape[0]-k):\n",
    "        new_arr[i,:,:] = arr[i-k:i+k+1,:]\n",
    "    for j in range(0,k):\n",
    "        new_arr[j,:,:] = new_arr[j+k,:,:]\n",
    "    for l in range(arr.shape[0]-k,arr.shape[0]):\n",
    "        new_arr[l,:,:] = new_arr[l-k,:,:]\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(arr,k):\n",
    "    arr1 = align_test(arr,k)\n",
    "    arr2 = xtest.reshape((arr1.shape[0], arr1.shape[1], arr1.shape[2],1))\n",
    "    predictions = model.predict(arr2,batch_size=32)\n",
    "    classes = np.argmax(predictions, axis = 1)\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_data(testx, 6)\n",
    "predict = pd.DataFrame({\"label\": predictions,})\n",
    "\n",
    "predict.to_csv('Speech_Recognition.csv', index_label=\"id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
